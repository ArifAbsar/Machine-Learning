{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import numpy as np\n",
    "import torch\n",
    "import my_utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as  models\n",
    "from torch.optim import Adam\n",
    "#from torchvision.models import vit_b_16, vit_t_16, vit_s_16  # For vision transformers\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "#from vision_transformer_cp \n",
    "from vision_transformer_cp import DINOHead, vit_small, vit_tiny, vit_base, vit_tinyer, vit_tiniest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "image_size = 32  # CIFAR-10 image size\n",
    "n_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "bag_size = 5 # Number of instances in each bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentationDINO(object):\n",
    "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number, image_size=224):\n",
    "        flip_and_color_jitter = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "        ])\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # CIFAR-10 mean\n",
    "                                 std=[0.2470, 0.2435, 0.2616])   # CIFAR-10 std\n",
    "        ])\n",
    "\n",
    "        # First global crop\n",
    "        self.global_transfo1 = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),  # Ensure images are resized\n",
    "            transforms.RandomResizedCrop(self.image_size, scale=global_crops_scale),\n",
    "            flip_and_color_jitter,\n",
    "            self.normalize,\n",
    "        ])\n",
    "        # Second global crop\n",
    "        self.global_transfo2 = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.RandomResizedCrop(self.image_size, scale=global_crops_scale),\n",
    "            flip_and_color_jitter,\n",
    "            self.normalize,\n",
    "        ])\n",
    "        # Transformation for the local small crops\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.local_transfo = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.RandomResizedCrop(self.image_size, scale=local_crops_scale),\n",
    "            flip_and_color_jitter,\n",
    "            self.normalize,\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        crops = []\n",
    "        # Generate global crops\n",
    "        crops.append(self.global_transfo1(image))\n",
    "        crops.append(self.global_transfo2(image))\n",
    "        # Generate local crops\n",
    "        for _ in range(self.local_crops_number):\n",
    "            crops.append(self.local_transfo(image))\n",
    "        # Transform the original image to tensor\n",
    "        unaugmented_image = transforms.Resize((self.image_size, self.image_size))(image)\n",
    "        unaugmented_image = self.normalize(unaugmented_image)\n",
    "       # print(f\"Unaugmented image shape: {unaugmented_image.shape}\")\n",
    "        #for idx, crop in enumerate(crops):\n",
    "           # print(f\"Crop {idx} shape: {crop.shape}\")\n",
    "        return unaugmented_image, crops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for data augmentation\n",
    "global_crops_scale = (0.4, 1.0)\n",
    "local_crops_scale = (0.05, 0.4)\n",
    "local_crops_number = 4  # Number of local crops\n",
    "image_size = 224  # Desired image size after resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_size=224\n",
    "data_transform = DataAugmentationDINO(\n",
    "    global_crops_scale=global_crops_scale,\n",
    "    local_crops_scale=local_crops_scale,\n",
    "    local_crops_number=local_crops_number,\n",
    "    image_size=image_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transform)\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagDataset(Dataset):\n",
    "    def __init__(self, dataset, bag_size=5):\n",
    "        self.dataset = dataset\n",
    "        self.bag_size = bag_size\n",
    "        self.indices = list(range(len(self.dataset)))\n",
    "        self.labels = [self.dataset.targets[i] for i in self.indices]\n",
    "        self.num_classes = len(set(self.labels))\n",
    "        print(f'Initializing BagDataset with {len(self.indices)} samples, bag size: {self.bag_size}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.bag_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get bag indices\n",
    "        start_idx = idx * self.bag_size\n",
    "        end_idx = start_idx + self.bag_size\n",
    "        bag_indices = self.indices[start_idx:end_idx]\n",
    "    \n",
    "        bag = []\n",
    "        labels = []\n",
    "        for i in bag_indices:\n",
    "            data, label = self.dataset[i]\n",
    "            unaugmented_image, crops = data\n",
    "            # Include both unaugmented image and crops\n",
    "            bag.append(unaugmented_image)\n",
    "            bag.extend(crops)\n",
    "            labels.append(label)\n",
    "    \n",
    "        # Convert bag and labels to tensors\n",
    "        bag = torch.stack(bag, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "    \n",
    "        # For MIL, assign the bag label (e.g., majority label or first label)\n",
    "        bag_label = labels[0]\n",
    "    \n",
    "        return bag, bag_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BagDataset with 50000 samples, bag size: 5\n",
      "Initializing BagDataset with 10000 samples, bag size: 5\n"
     ]
    }
   ],
   "source": [
    "# Create BagDatasets\n",
    "train_bag_dataset = BagDataset(train_dataset, bag_size=bag_size)\n",
    "val_bag_dataset = BagDataset(val_dataset, bag_size=bag_size)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_bag_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True)\n",
    "val_loader = DataLoader(val_bag_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(nn.Module):\n",
    "    def __init__(self, out_dim=500, use_bn=False, model_type=\"tiny\"):\n",
    "        super().__init__()\n",
    "        model_map = {'tiny':vit_tiny(), 'small':vit_small(), 'base':vit_base(),\n",
    "                     'vit_tinyer':vit_tinyer()\n",
    "                    }\n",
    "        # Student network\n",
    "        self.student = model_map[model_type]\n",
    "        embed_dim = self.student.embed_dim\n",
    "        \n",
    "        self.student = nn.Sequential(\n",
    "            self.student,\n",
    "            DINOHead(embed_dim, out_dim, use_bn)\n",
    "        )\n",
    "        # Teacher network\n",
    "        self.teacher = model_map[model_type]\n",
    "        self.teacher = nn.Sequential(\n",
    "            self.teacher,\n",
    "            DINOHead(embed_dim, out_dim, use_bn)\n",
    "        )\n",
    "        # Initialize teacher and student with same weights\n",
    "        self.teacher.load_state_dict(self.student.state_dict())\n",
    "        # Turn off gradients for teacher network\n",
    "        for param in self.teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, is_teacher=False):\n",
    "        # Forward pass through Perceiver\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.student[0](x) if not is_teacher else self.teacher[0](x)\n",
    "        # combine latents\n",
    "        x = x.view(batch_size, 1, -1)\n",
    "        x = self.student[1](x) if not is_teacher else self.teacher[1](x)\n",
    "        return x\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        return self.student[0].get_last_selfattention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_classes),\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(InstanceClassifier, self).__init__()\n",
    "        self.classifier = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        batch_size, num_instances, feature_dim = x.size()\n",
    "        x = x.view(-1, feature_dim)\n",
    "        predictions = self.classifier(x)\n",
    "        predictions = predictions.view(batch_size, num_instances, -1)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Attention Mechanism\n",
    "class GatedAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GatedAttention, self).__init__()\n",
    "        self.M = input_dim\n",
    "        self.L = 128\n",
    "        self.ATTENTION_BRANCHES = 1\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(input_dim, self.M),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_w = nn.Linear(self.L, self.ATTENTION_BRANCHES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        batch_size, num_instances, feature_dim = x.size()\n",
    "        x = x.view(batch_size * num_instances, feature_dim)\n",
    "\n",
    "        # Apply feature_extractor_part2\n",
    "        H = self.feature_extractor_part2(x)\n",
    "\n",
    "        # Compute attention weights\n",
    "        A_V = self.attention_V(H)\n",
    "        A_U = self.attention_U(H)\n",
    "        A = self.attention_w(A_V * A_U)\n",
    "\n",
    "        # Reshape for softmax\n",
    "        A = A.view(batch_size, num_instances, self.ATTENTION_BRANCHES)\n",
    "        A = A.transpose(1, 2)\n",
    "        A = F.softmax(A, dim=2)\n",
    "\n",
    "        # Reshape H\n",
    "        H = H.view(batch_size, num_instances, -1)\n",
    "\n",
    "        # Compute bag representation\n",
    "        Z = torch.bmm(A, H)\n",
    "\n",
    "        if self.ATTENTION_BRANCHES == 1:\n",
    "            Z = Z.squeeze(1)\n",
    "        return Z, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MILModel(nn.Module):\n",
    "    def __init__(self, feature_extractor, aggregator, classifier):\n",
    "        super(MILModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.aggregator = aggregator\n",
    "        self.classifier = classifier  # InstanceClassifier\n",
    "\n",
    "    def forward(self, bag):\n",
    "        batch_size, num_instances, channels, height, width = bag.shape\n",
    "        bag = bag.view(-1, channels, height, width)\n",
    "        features = self.feature_extractor(bag)\n",
    "        features = features.view(batch_size, num_instances, -1)\n",
    "\n",
    "        # Get instance-level predictions\n",
    "        instance_predictions = self.classifier(features)  # Shape: (batch_size, num_instances, num_classes)\n",
    "\n",
    "        # Get attention weights\n",
    "        bag_representation, attention_weights = self.aggregator(features)  # attention_weights shape: (batch_size, num_attention_branches, num_instances)\n",
    "\n",
    "        # Use attention weights to compute weighted sum of instance predictions\n",
    "        attention_weights = attention_weights.squeeze(1)  # Shape: (batch_size, num_instances)\n",
    "        attention_weights = attention_weights.unsqueeze(2)  # Shape: (batch_size, num_instances, 1)\n",
    "        weighted_predictions = (instance_predictions * attention_weights).sum(dim=1)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return weighted_predictions, attention_weights,instance_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Machine learning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "f:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "f:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "# Number of classes in CIFAR-10\n",
    "num_classes = 10\n",
    "\n",
    "#model_name='vit_small_patch16_224_dino'\n",
    "\n",
    "# Initialize model components\n",
    "feature_extractor = DINO(out_dim=768, use_bn=False, model_type='base')\n",
    "#model=models.resnet50(pretrained=True)\n",
    "\n",
    "# Remove the classification head to get feature vectors\n",
    "#feature_extractor.reset_classifier(0)\n",
    "#feature_extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "aggregator = GatedAttention(input_dim=768).to(device)\n",
    "#classifier = Classifier(input_dim=2048, num_classes=num_classes).to(device)\n",
    "classifier = InstanceClassifier(input_dim=768, num_classes=num_classes).to(device)\n",
    "model = MILModel(feature_extractor, aggregator, classifier).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "scaler = torch.amp.GradScaler(device='cuda')\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch_idx, (bags, bag_labels) in enumerate(train_loader):\n",
    "        bags = bags.to(device)\n",
    "        bag_labels = bag_labels.to(device)\n",
    "        print(f'Processing Batch {batch_idx + 1}/{len(train_loader)}')\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=True,device_type='cuda'):\n",
    "            outputs, attention_weights,_ = model(bags)\n",
    "            loss = criterion(outputs, bag_labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        torch.cuda.empty_cache()  \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == bag_labels).sum().item()\n",
    "        total_train += bag_labels.size(0)\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Batch {batch_idx + 1}/{len(train_loader)}: Loss = {loss.item():.4f}')\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return train_accuracy, average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    images_list = []\n",
    "    attention_weights_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bags, bag_labels in val_loader:\n",
    "            bags = bags.to(device)\n",
    "            bag_labels = bag_labels.to(device)\n",
    "\n",
    "            with autocast(enabled=True,device_type='cuda'):\n",
    "                outputs, attention_weights, _ = model(bags)  # Unpack three values\n",
    "                loss = criterion(outputs, bag_labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == bag_labels).sum().item()\n",
    "            total += bag_labels.size(0)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(bag_labels.cpu().numpy())\n",
    "            images_list.extend(bags.cpu())\n",
    "            attention_weights_list.extend(attention_weights.cpu())\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(val_loader)\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "    print(f'Validation Loss: {average_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%, F1 Score: {f1:.4f}')\n",
    "    return val_accuracy, images_list, all_predictions, all_targets, attention_weights_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(images_list, attention_weights_list, class_names, num_images=5, epoch=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Limit the number of images to display\n",
    "    num_images = min(num_images, len(images_list))\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        bag = images_list[idx]  # Shape: (num_instances, 3, 224, 224)\n",
    "        attention_weights = attention_weights_list[idx]  # Shape: (num_instances, 1)\n",
    "\n",
    "        # Ensure attention_weights is a 1D array\n",
    "        attention_weights = attention_weights.squeeze().cpu().numpy()\n",
    "\n",
    "        # Sort instances by attention weight in descending order\n",
    "        sorted_indices = np.argsort(-attention_weights)\n",
    "        top_instances = bag[sorted_indices][:5]  # Top 5 instances\n",
    "        top_weights = attention_weights[sorted_indices][:5]\n",
    "\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        for i in range(len(top_instances)):\n",
    "            plt.subplot(1, 5, i+1)\n",
    "            image = top_instances[i]\n",
    "            image = image.permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "            # Un-normalize the image\n",
    "            image = image * np.array([0.2470, 0.2435, 0.2616]) + np.array([0.4914, 0.4822, 0.4465])\n",
    "            image = np.clip(image, 0, 1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f'Weight: {top_weights[i]:.2f}')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if epoch is not None:\n",
    "            plt.savefig(f'attention_epoch_{epoch + 1}_bag_{idx + 1}.png')  # Save plot\n",
    "        plt.show()\n",
    "        plt.close()  # Close the figure to free memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def visualize_predictions(images_list, predictions, targets, class_names, num_images=10):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Limit the number of images to display\n",
    "    num_images = min(num_images, len(images_list))\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for idx in range(num_images):\n",
    "        plt.subplot(2, 5, idx+1)\n",
    "        # images_list contains bags; extract one image per bag for visualization\n",
    "        image = images_list[idx][0]  # Take the first image from the bag\n",
    "        image = image.permute(1, 2, 0).numpy()  # Convert tensor to numpy array\n",
    "        # Un-normalize the image\n",
    "        image = image * np.array([0.2470, 0.2435, 0.2616]) + np.array([0.4914, 0.4822, 0.4465])\n",
    "        image = np.clip(image, 0, 1)\n",
    "        plt.imshow(image)\n",
    "        pred_label = class_names[predictions[idx]]\n",
    "        true_label = class_names[targets[idx]]\n",
    "        plt.title(f\"Predicted: {pred_label}\\nActual: {true_label}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_names = train_dataset.classes  # ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Processing Batch 1/157\n",
      "Processing Batch 2/157\n",
      "Processing Batch 3/157\n",
      "Processing Batch 4/157\n",
      "Processing Batch 5/157\n",
      "Processing Batch 6/157\n",
      "Processing Batch 7/157\n",
      "Processing Batch 8/157\n",
      "Processing Batch 9/157\n",
      "Processing Batch 10/157\n",
      "Processing Batch 11/157\n",
      "Processing Batch 12/157\n",
      "Processing Batch 13/157\n",
      "Processing Batch 14/157\n",
      "Processing Batch 15/157\n",
      "Processing Batch 16/157\n",
      "Processing Batch 17/157\n",
      "Processing Batch 18/157\n",
      "Processing Batch 19/157\n",
      "Processing Batch 20/157\n",
      "Processing Batch 21/157\n",
      "Processing Batch 22/157\n",
      "Processing Batch 23/157\n",
      "Processing Batch 24/157\n",
      "Processing Batch 25/157\n",
      "Processing Batch 26/157\n",
      "Processing Batch 27/157\n",
      "Processing Batch 28/157\n",
      "Processing Batch 29/157\n",
      "Processing Batch 30/157\n",
      "Processing Batch 31/157\n",
      "Processing Batch 32/157\n",
      "Processing Batch 33/157\n",
      "Processing Batch 34/157\n",
      "Processing Batch 35/157\n",
      "Processing Batch 36/157\n",
      "Processing Batch 37/157\n",
      "Processing Batch 38/157\n",
      "Processing Batch 39/157\n",
      "Processing Batch 40/157\n",
      "Processing Batch 41/157\n",
      "Processing Batch 42/157\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train_accuracy, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     val_accuracy, val_images_list, val_predictions, val_targets, attention_weights_list \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)    \n",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     outputs, attention_weights,_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, bag_labels)\n\u001b[0;32m     19\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mMILModel.forward\u001b[1;34m(self, bag)\u001b[0m\n\u001b[0;32m      9\u001b[0m batch_size, num_instances, channels, height, width \u001b[38;5;241m=\u001b[39m bag\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     10\u001b[0m bag \u001b[38;5;241m=\u001b[39m bag\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, channels, height, width)\n\u001b[1;32m---> 11\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mview(batch_size, num_instances, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Get instance-level predictions\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mDINO.forward\u001b[1;34m(self, x, is_teacher)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, is_teacher\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Forward pass through Perceiver\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_teacher \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteacher[\u001b[38;5;241m0\u001b[39m](x)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# combine latents\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Machine learning\\vision_transformer_cp.py:212\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    210\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_tokens(x)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Machine learning\\vision_transformer_cp.py:108\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x, return_attention)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 108\u001b[0m     y, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_attention:\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Machine learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Machine learning\\vision_transformer_cp.py:89\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     87\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n\u001b[1;32m---> 89\u001b[0m x \u001b[38;5;241m=\u001b[39m (\u001b[43mattn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, N, C)\n\u001b[0;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n\u001b[0;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_drop(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}]')\n",
    "    train_accuracy, train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    val_accuracy, val_images_list, val_predictions, val_targets, attention_weights_list = validate(model, val_loader, criterion, device)    \n",
    "    # Visualize predictions after validation\n",
    "    #visualize_attention(val_images_list, attention_weights_list, class_names, num_images=5)\n",
    "    visualize_predictions(val_images_list, val_predictions, val_targets, class_names, num_images=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
